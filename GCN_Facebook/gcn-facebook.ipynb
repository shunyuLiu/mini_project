{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data from data set.\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    cat_data = np.load(\"facebook.npz\")\n",
    "    n = len(cat_data[\"target\"])\n",
    "    x = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in cat_data[\"edges\"]:\n",
    "        x[i[0]][i[1]] = 1\n",
    "    return sp.csr_matrix(x), sp.csr_matrix(cat_data[\"features\"], dtype=np.float32), cat_data[\"target\"]\n",
    "\n",
    "\"\"\"\n",
    "normalize the data to 1.\n",
    "\"\"\"\n",
    "def normalize_adj(adjacency):\n",
    "    adjacency += sp.eye(adjacency.shape[0])\n",
    "    degree = np.array(adjacency.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    return d_hat.dot(adjacency).dot(d_hat).tocoo()\n",
    "\n",
    "def normalize_features(features):\n",
    "    return features / features.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5000000000000001\n",
      "  (0, 18427)\t0.09805806756909202\n",
      "  (1, 1)\t0.028571428571428567\n",
      "  (1, 2812)\t0.02492223931396134\n",
      "  (1, 4987)\t0.016903085094570332\n",
      "  (1, 5228)\t0.020965696734438363\n",
      "  (1, 5307)\t0.015240998561973751\n",
      "  (1, 5755)\t0.020348923188911988\n",
      "  (1, 6829)\t0.031943828249996996\n",
      "  (1, 7136)\t0.033149677206589796\n",
      "  (1, 8049)\t0.012135707849456652\n",
      "  (1, 8533)\t0.027788500718836418\n",
      "  (1, 8894)\t0.023218173010628604\n",
      "  (1, 9934)\t0.019920476822239894\n",
      "  (1, 10281)\t0.04517539514526256\n",
      "  (1, 10379)\t0.007805119495830757\n",
      "  (1, 10554)\t0.02238868314198225\n",
      "  (1, 11557)\t0.019783564706223267\n",
      "  (1, 12305)\t0.020498001542269693\n",
      "  (1, 13737)\t0.021295885499998\n",
      "  (1, 14344)\t0.02366905341655754\n",
      "  (1, 15026)\t0.02577696311132335\n",
      "  (1, 15785)\t0.013894250359418209\n",
      "  (1, 16260)\t0.03253000243161777\n",
      "  (1, 16590)\t0.018018749253911177\n",
      "  :\t:\n",
      "  (22467, 5339)\t0.038235955645093626\n",
      "  (22467, 6181)\t0.04233337566673017\n",
      "  (22467, 8565)\t0.03984095364447979\n",
      "  (22467, 9367)\t0.02703690352179376\n",
      "  (22467, 9986)\t0.03367175148507369\n",
      "  (22467, 10347)\t0.048112522432468816\n",
      "  (22467, 10775)\t0.04376881095324085\n",
      "  (22467, 11187)\t0.04622501635210243\n",
      "  (22467, 11942)\t0.050251890762960605\n",
      "  (22467, 13121)\t0.048112522432468816\n",
      "  (22467, 16023)\t0.04103049699311091\n",
      "  (22467, 16564)\t0.0445435403187374\n",
      "  (22467, 16813)\t0.040422604172722164\n",
      "  (22467, 22467)\t0.05555555555555555\n",
      "  (22468, 640)\t0.3333333333333333\n",
      "  (22468, 4459)\t0.2041241452319315\n",
      "  (22468, 22468)\t0.3333333333333333\n",
      "  (22469, 2009)\t0.14433756729740646\n",
      "  (22469, 2062)\t0.15811388300841897\n",
      "  (22469, 7699)\t0.14433756729740646\n",
      "  (22469, 10823)\t0.14433756729740646\n",
      "  (22469, 11417)\t0.25000000000000006\n",
      "  (22469, 14921)\t0.12500000000000003\n",
      "  (22469, 19369)\t0.12500000000000003\n",
      "  (22469, 22469)\t0.12500000000000003\n",
      "tensor([[ 0.0210,  0.0222,  0.0210,  ...,  0.0172,  0.0301,  0.0179],\n",
      "        [ 0.0104,  0.0109,  0.0103,  ...,  0.0085,  0.0144,  0.0051],\n",
      "        [-0.0101, -0.0102, -0.0101,  ..., -0.0083, -0.0144, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0130,  0.0137,  0.0130,  ...,  0.0090,  0.0185,  0.0111],\n",
      "        [ 0.0119,  0.0126,  0.0119,  ...,  0.0098,  0.0171,  0.0099],\n",
      "        [ 0.0084,  0.0099,  0.0094,  ...,  0.0070,  0.0135,  0.0080]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d6d3a1dc35b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjacency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "adjacency, features, labels = load_data()\n",
    "encode_onehot = LabelBinarizer()\n",
    "labels = encode_onehot.fit_transform(labels)\n",
    "\n",
    "adjacency = normalize_adj(adjacency)\n",
    "features = normalize_features(features)\n",
    "features = torch.FloatTensor(np.array(features))\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "print(adjacency)\n",
    "print(features)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470\n"
     ]
    }
   ],
   "source": [
    "print(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = features.shape[0]\n",
    "train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "train_mask[:140] = True\n",
    "val_mask[200:500] = True\n",
    "test_mask[500:1500] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, adjacency, input_feature):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        support = torch.mm(input_feature, self.weight.to(device))\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias.to(device)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(input_dim, 16)\n",
    "        self.gcn2 = GCNLayer(16, 4)\n",
    "    \n",
    "    def forward(self, adjacency, feature):\n",
    "        h = fun.relu(self.gcn1(adjacency, feature))\n",
    "        logits = self.gcn2(adjacency, h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "epochs = 200\n",
    "\n",
    "device = \"cpu\"\n",
    "model = GCN().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "tensor_x = features.to(device)\n",
    "tensor_y = labels.to(device)\n",
    "tensor_train_mask = torch.from_numpy(train_mask).to(device)\n",
    "tensor_val_mask = torch.from_numpy(val_mask).to(device)\n",
    "tensor_test_mask = torch.from_numpy(test_mask).to(device)\n",
    "indices = torch.from_numpy(np.asarray([adjacency.row, adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(adjacency.data.astype(np.float32))\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, (22470, 22470)).to(device)\n",
    "\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        test_mask_logits = logits[mask]\n",
    "        predict_y = test_mask_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
    "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_y[mask].cpu().numpy()\n",
    "\n",
    "def train():\n",
    "    loss_history = []\n",
    "    val_acc_history = []\n",
    "    model.train()\n",
    "    train_y = tensor_y[tensor_train_mask]\n",
    "    for epoch in range(epochs):\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        train_mask_logits = logits[tensor_train_mask]\n",
    "        loss = criterion(train_mask_logits, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc, _, _ = test(tensor_train_mask)\n",
    "        val_acc, _, _ = test(tensor_val_mask)\n",
    "        loss_history.append(loss.item())\n",
    "        val_acc_history.append(val_acc.item())\n",
    "        print(\"Epoch {:03d}: Loss {:.4f}, TrainAcc {:.4}, ValAcc {:.4f}\".format(\n",
    "            epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "    \n",
    "    return loss_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss 1.3964, TrainAcc 0.4143, ValAcc 0.3200\n",
      "Epoch 001: Loss 1.2400, TrainAcc 0.5571, ValAcc 0.4333\n",
      "Epoch 002: Loss 1.1603, TrainAcc 0.6286, ValAcc 0.4967\n",
      "Epoch 003: Loss 1.0641, TrainAcc 0.6214, ValAcc 0.4833\n",
      "Epoch 004: Loss 0.9598, TrainAcc 0.65, ValAcc 0.5167\n",
      "Epoch 005: Loss 0.8565, TrainAcc 0.8071, ValAcc 0.6067\n",
      "Epoch 006: Loss 0.7593, TrainAcc 0.8786, ValAcc 0.6900\n",
      "Epoch 007: Loss 0.6604, TrainAcc 0.8857, ValAcc 0.6733\n",
      "Epoch 008: Loss 0.5695, TrainAcc 0.8929, ValAcc 0.6900\n",
      "Epoch 009: Loss 0.4889, TrainAcc 0.8857, ValAcc 0.7000\n",
      "Epoch 010: Loss 0.4165, TrainAcc 0.95, ValAcc 0.7167\n",
      "Epoch 011: Loss 0.3514, TrainAcc 0.9571, ValAcc 0.7300\n",
      "Epoch 012: Loss 0.3020, TrainAcc 0.9571, ValAcc 0.7300\n",
      "Epoch 013: Loss 0.2564, TrainAcc 0.9571, ValAcc 0.7300\n",
      "Epoch 014: Loss 0.2199, TrainAcc 0.9643, ValAcc 0.7333\n",
      "Epoch 015: Loss 0.1890, TrainAcc 0.9714, ValAcc 0.7200\n",
      "Epoch 016: Loss 0.1651, TrainAcc 0.9643, ValAcc 0.7500\n",
      "Epoch 017: Loss 0.1432, TrainAcc 0.9786, ValAcc 0.7600\n",
      "Epoch 018: Loss 0.1283, TrainAcc 0.9929, ValAcc 0.7600\n",
      "Epoch 019: Loss 0.1163, TrainAcc 0.9857, ValAcc 0.7533\n",
      "Epoch 020: Loss 0.1049, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 021: Loss 0.0971, TrainAcc 0.9929, ValAcc 0.7567\n",
      "Epoch 022: Loss 0.0902, TrainAcc 0.9857, ValAcc 0.7667\n",
      "Epoch 023: Loss 0.0862, TrainAcc 0.9929, ValAcc 0.7667\n",
      "Epoch 024: Loss 0.0819, TrainAcc 0.9929, ValAcc 0.7833\n",
      "Epoch 025: Loss 0.0804, TrainAcc 0.9929, ValAcc 0.7800\n",
      "Epoch 026: Loss 0.0792, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 027: Loss 0.0787, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 028: Loss 0.0792, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 029: Loss 0.0809, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 030: Loss 0.0827, TrainAcc 1.0, ValAcc 0.7800\n",
      "Epoch 031: Loss 0.0850, TrainAcc 1.0, ValAcc 0.7733\n",
      "Epoch 032: Loss 0.0874, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 033: Loss 0.0902, TrainAcc 1.0, ValAcc 0.7733\n",
      "Epoch 034: Loss 0.0923, TrainAcc 1.0, ValAcc 0.7733\n",
      "Epoch 035: Loss 0.0948, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 036: Loss 0.0967, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 037: Loss 0.0982, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 038: Loss 0.1000, TrainAcc 0.9929, ValAcc 0.7767\n",
      "Epoch 039: Loss 0.1041, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 040: Loss 0.1088, TrainAcc 0.9857, ValAcc 0.7700\n",
      "Epoch 041: Loss 0.1099, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 042: Loss 0.0984, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 043: Loss 0.0993, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 044: Loss 0.1002, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 045: Loss 0.0898, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 046: Loss 0.0936, TrainAcc 0.9929, ValAcc 0.7667\n",
      "Epoch 047: Loss 0.0881, TrainAcc 0.9929, ValAcc 0.7667\n",
      "Epoch 048: Loss 0.0842, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 049: Loss 0.0859, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 050: Loss 0.0789, TrainAcc 0.9929, ValAcc 0.7667\n",
      "Epoch 051: Loss 0.0812, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 052: Loss 0.0773, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 053: Loss 0.0771, TrainAcc 0.9929, ValAcc 0.7667\n",
      "Epoch 054: Loss 0.0772, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 055: Loss 0.0746, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 056: Loss 0.0769, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 057: Loss 0.0749, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 058: Loss 0.0757, TrainAcc 1.0, ValAcc 0.7733\n",
      "Epoch 059: Loss 0.0764, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 060: Loss 0.0756, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 061: Loss 0.0771, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 062: Loss 0.0765, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 063: Loss 0.0768, TrainAcc 1.0, ValAcc 0.7733\n",
      "Epoch 064: Loss 0.0780, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 065: Loss 0.0769, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 066: Loss 0.0774, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 067: Loss 0.0780, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 068: Loss 0.0769, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 069: Loss 0.0770, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 070: Loss 0.0772, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 071: Loss 0.0762, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 072: Loss 0.0760, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 073: Loss 0.0759, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 074: Loss 0.0750, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 075: Loss 0.0746, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 076: Loss 0.0747, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 077: Loss 0.0739, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 078: Loss 0.0735, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 079: Loss 0.0737, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 080: Loss 0.0734, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 081: Loss 0.0730, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 082: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 083: Loss 0.0732, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 084: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 085: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 086: Loss 0.0733, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 087: Loss 0.0732, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 088: Loss 0.0732, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 089: Loss 0.0733, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 090: Loss 0.0733, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 091: Loss 0.0733, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 092: Loss 0.0732, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 093: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 094: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 095: Loss 0.0730, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 096: Loss 0.0729, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 097: Loss 0.0728, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 098: Loss 0.0727, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 099: Loss 0.0727, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 100: Loss 0.0726, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 101: Loss 0.0725, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 102: Loss 0.0724, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 103: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 104: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 105: Loss 0.0722, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 106: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 107: Loss 0.0722, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 108: Loss 0.0725, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 109: Loss 0.0727, TrainAcc 0.9929, ValAcc 0.7500\n",
      "Epoch 110: Loss 0.0737, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 111: Loss 0.0747, TrainAcc 0.9929, ValAcc 0.7600\n",
      "Epoch 112: Loss 0.0779, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 113: Loss 0.0795, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 114: Loss 0.0837, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 115: Loss 0.0764, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 116: Loss 0.0705, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 117: Loss 0.0713, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 118: Loss 0.0738, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 119: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 120: Loss 0.0689, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 121: Loss 0.0712, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 122: Loss 0.0734, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 123: Loss 0.0701, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 124: Loss 0.0702, TrainAcc 0.9929, ValAcc 0.7533\n",
      "Epoch 125: Loss 0.0729, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 126: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 127: Loss 0.0707, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 128: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 129: Loss 0.0728, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 130: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 131: Loss 0.0716, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 132: Loss 0.0725, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 133: Loss 0.0733, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 134: Loss 0.0724, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 135: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 136: Loss 0.0724, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 137: Loss 0.0728, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 138: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 139: Loss 0.0717, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 140: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 141: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 142: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 143: Loss 0.0715, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 144: Loss 0.0717, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 145: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 146: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 147: Loss 0.0715, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 148: Loss 0.0716, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 149: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 150: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 151: Loss 0.0717, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 152: Loss 0.0716, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 153: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 154: Loss 0.0720, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 155: Loss 0.0720, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 156: Loss 0.0723, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 157: Loss 0.0727, TrainAcc 1.0, ValAcc 0.7767\n",
      "Epoch 158: Loss 0.0736, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 159: Loss 0.0748, TrainAcc 0.9929, ValAcc 0.7733\n",
      "Epoch 160: Loss 0.0765, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 161: Loss 0.0773, TrainAcc 0.9929, ValAcc 0.7633\n",
      "Epoch 162: Loss 0.0777, TrainAcc 1.0, ValAcc 0.7700\n",
      "Epoch 163: Loss 0.0741, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 164: Loss 0.0706, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 165: Loss 0.0694, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 166: Loss 0.0713, TrainAcc 0.9929, ValAcc 0.7567\n",
      "Epoch 167: Loss 0.0731, TrainAcc 1.0, ValAcc 0.7667\n",
      "Epoch 168: Loss 0.0714, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 169: Loss 0.0695, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 170: Loss 0.0695, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 171: Loss 0.0712, TrainAcc 0.9929, ValAcc 0.7600\n",
      "Epoch 172: Loss 0.0727, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 173: Loss 0.0718, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 174: Loss 0.0708, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 175: Loss 0.0707, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 176: Loss 0.0715, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 177: Loss 0.0725, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 178: Loss 0.0725, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 179: Loss 0.0720, TrainAcc 1.0, ValAcc 0.7533\n",
      "Epoch 180: Loss 0.0715, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 181: Loss 0.0713, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 182: Loss 0.0717, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 183: Loss 0.0720, TrainAcc 1.0, ValAcc 0.7633\n",
      "Epoch 184: Loss 0.0722, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 185: Loss 0.0719, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 186: Loss 0.0714, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 187: Loss 0.0710, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 188: Loss 0.0709, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 189: Loss 0.0711, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 190: Loss 0.0714, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 191: Loss 0.0715, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 192: Loss 0.0714, TrainAcc 1.0, ValAcc 0.7500\n",
      "Epoch 193: Loss 0.0713, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 194: Loss 0.0711, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 195: Loss 0.0711, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 196: Loss 0.0710, TrainAcc 1.0, ValAcc 0.7567\n",
      "Epoch 197: Loss 0.0711, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 198: Loss 0.0712, TrainAcc 1.0, ValAcc 0.7600\n",
      "Epoch 199: Loss 0.0712, TrainAcc 1.0, ValAcc 0.7533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.3963514566421509,\n",
       "  1.24004065990448,\n",
       "  1.1602709293365479,\n",
       "  1.0641227960586548,\n",
       "  0.9598279595375061,\n",
       "  0.8564662933349609,\n",
       "  0.7592632174491882,\n",
       "  0.6604185104370117,\n",
       "  0.5695254802703857,\n",
       "  0.4889271855354309,\n",
       "  0.4165275990962982,\n",
       "  0.3514234125614166,\n",
       "  0.3019949495792389,\n",
       "  0.2564278244972229,\n",
       "  0.21985819935798645,\n",
       "  0.1890411376953125,\n",
       "  0.16505202651023865,\n",
       "  0.1432422250509262,\n",
       "  0.12829263508319855,\n",
       "  0.11634216457605362,\n",
       "  0.10494226217269897,\n",
       "  0.09705748409032822,\n",
       "  0.0902150422334671,\n",
       "  0.08619112521409988,\n",
       "  0.08187592029571533,\n",
       "  0.0803593248128891,\n",
       "  0.07916141301393509,\n",
       "  0.07867401838302612,\n",
       "  0.07922174036502838,\n",
       "  0.0808560773730278,\n",
       "  0.08274457603693008,\n",
       "  0.08499888330698013,\n",
       "  0.08741258084774017,\n",
       "  0.09021323919296265,\n",
       "  0.09229430556297302,\n",
       "  0.09475058317184448,\n",
       "  0.09671872854232788,\n",
       "  0.09823983907699585,\n",
       "  0.10003148019313812,\n",
       "  0.10410627722740173,\n",
       "  0.10882759839296341,\n",
       "  0.10988621413707733,\n",
       "  0.09835580736398697,\n",
       "  0.09932655841112137,\n",
       "  0.10015419870615005,\n",
       "  0.08978889137506485,\n",
       "  0.09359578788280487,\n",
       "  0.08806706964969635,\n",
       "  0.0841570645570755,\n",
       "  0.08585777133703232,\n",
       "  0.0788809061050415,\n",
       "  0.0812084972858429,\n",
       "  0.0772523432970047,\n",
       "  0.0770631805062294,\n",
       "  0.0772189050912857,\n",
       "  0.0746343582868576,\n",
       "  0.07685986161231995,\n",
       "  0.07490607351064682,\n",
       "  0.07571247965097427,\n",
       "  0.07636472582817078,\n",
       "  0.07559003680944443,\n",
       "  0.07706934213638306,\n",
       "  0.07646558433771133,\n",
       "  0.07680454850196838,\n",
       "  0.07796426117420197,\n",
       "  0.07693156599998474,\n",
       "  0.07744777202606201,\n",
       "  0.07801267504692078,\n",
       "  0.07689233869314194,\n",
       "  0.07704322040081024,\n",
       "  0.07716277986764908,\n",
       "  0.07617906481027603,\n",
       "  0.07598157972097397,\n",
       "  0.07588628679513931,\n",
       "  0.07496178895235062,\n",
       "  0.07463548332452774,\n",
       "  0.07465262711048126,\n",
       "  0.07391565293073654,\n",
       "  0.07352542877197266,\n",
       "  0.07367877662181854,\n",
       "  0.07335154712200165,\n",
       "  0.07304603606462479,\n",
       "  0.07313326001167297,\n",
       "  0.0731944665312767,\n",
       "  0.07306119799613953,\n",
       "  0.07308441400527954,\n",
       "  0.07329722493886948,\n",
       "  0.07323379814624786,\n",
       "  0.07316718995571136,\n",
       "  0.07325918972492218,\n",
       "  0.07333000004291534,\n",
       "  0.0733112171292305,\n",
       "  0.07315289974212646,\n",
       "  0.07311484962701797,\n",
       "  0.07311347872018814,\n",
       "  0.07301093637943268,\n",
       "  0.0728914737701416,\n",
       "  0.07276106625795364,\n",
       "  0.07270889729261398,\n",
       "  0.07266149669885635,\n",
       "  0.07257822155952454,\n",
       "  0.07245528697967529,\n",
       "  0.0723724365234375,\n",
       "  0.07227783650159836,\n",
       "  0.07228224724531174,\n",
       "  0.07221421599388123,\n",
       "  0.07227162271738052,\n",
       "  0.07224640995264053,\n",
       "  0.07248697429895401,\n",
       "  0.07271739840507507,\n",
       "  0.07367254793643951,\n",
       "  0.074722059071064,\n",
       "  0.07794031500816345,\n",
       "  0.07951994240283966,\n",
       "  0.0836789458990097,\n",
       "  0.07638396322727203,\n",
       "  0.07049468904733658,\n",
       "  0.07126699388027191,\n",
       "  0.07376676052808762,\n",
       "  0.07232040166854858,\n",
       "  0.06894802302122116,\n",
       "  0.07118510454893112,\n",
       "  0.07343078404664993,\n",
       "  0.07013935595750809,\n",
       "  0.07022931426763535,\n",
       "  0.0729260966181755,\n",
       "  0.07183035463094711,\n",
       "  0.07074932754039764,\n",
       "  0.07187042385339737,\n",
       "  0.0728059634566307,\n",
       "  0.07231467217206955,\n",
       "  0.07156575471162796,\n",
       "  0.07248608767986298,\n",
       "  0.07332827150821686,\n",
       "  0.07236167788505554,\n",
       "  0.07178866863250732,\n",
       "  0.0724279060959816,\n",
       "  0.07276161760091782,\n",
       "  0.07233226299285889,\n",
       "  0.0717039555311203,\n",
       "  0.07191219925880432,\n",
       "  0.07233937829732895,\n",
       "  0.07188564538955688,\n",
       "  0.07153204828500748,\n",
       "  0.07167953997850418,\n",
       "  0.07189309597015381,\n",
       "  0.07190117239952087,\n",
       "  0.07154311239719391,\n",
       "  0.07155284285545349,\n",
       "  0.07183559238910675,\n",
       "  0.07182686030864716,\n",
       "  0.07172306627035141,\n",
       "  0.07162360101938248,\n",
       "  0.07175058871507645,\n",
       "  0.07200691103935242,\n",
       "  0.07202746719121933,\n",
       "  0.07227559387683868,\n",
       "  0.0727148950099945,\n",
       "  0.0736321210861206,\n",
       "  0.0747886374592781,\n",
       "  0.07649292051792145,\n",
       "  0.07730595767498016,\n",
       "  0.07773657888174057,\n",
       "  0.07410986721515656,\n",
       "  0.07061848789453506,\n",
       "  0.06939055770635605,\n",
       "  0.07129789143800735,\n",
       "  0.07307837158441544,\n",
       "  0.07136513292789459,\n",
       "  0.06952085345983505,\n",
       "  0.06947232037782669,\n",
       "  0.07120179384946823,\n",
       "  0.07268662005662918,\n",
       "  0.0718456506729126,\n",
       "  0.07084906101226807,\n",
       "  0.07068971544504166,\n",
       "  0.07152293622493744,\n",
       "  0.07254862040281296,\n",
       "  0.07251252233982086,\n",
       "  0.0720396339893341,\n",
       "  0.07145529985427856,\n",
       "  0.07128476351499557,\n",
       "  0.07165911793708801,\n",
       "  0.07201876491308212,\n",
       "  0.07223255932331085,\n",
       "  0.0719020813703537,\n",
       "  0.07139135897159576,\n",
       "  0.07099688798189163,\n",
       "  0.07093846052885056,\n",
       "  0.07114177197217941,\n",
       "  0.07137361168861389,\n",
       "  0.07149560004472733,\n",
       "  0.07139185816049576,\n",
       "  0.07128185033798218,\n",
       "  0.07109984010457993,\n",
       "  0.07106269896030426,\n",
       "  0.07102122157812119,\n",
       "  0.07107044011354446,\n",
       "  0.0711558610200882,\n",
       "  0.07124081999063492],\n",
       " [0.3199999928474426,\n",
       "  0.4333333373069763,\n",
       "  0.49666666984558105,\n",
       "  0.4833333194255829,\n",
       "  0.5166666507720947,\n",
       "  0.6066666841506958,\n",
       "  0.6899999976158142,\n",
       "  0.6733333468437195,\n",
       "  0.6899999976158142,\n",
       "  0.699999988079071,\n",
       "  0.7166666388511658,\n",
       "  0.7300000190734863,\n",
       "  0.7300000190734863,\n",
       "  0.7300000190734863,\n",
       "  0.7333333492279053,\n",
       "  0.7200000286102295,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.7833333611488342,\n",
       "  0.7799999713897705,\n",
       "  0.7766666412353516,\n",
       "  0.7766666412353516,\n",
       "  0.7666666507720947,\n",
       "  0.7699999809265137,\n",
       "  0.7799999713897705,\n",
       "  0.7733333110809326,\n",
       "  0.7766666412353516,\n",
       "  0.7733333110809326,\n",
       "  0.7733333110809326,\n",
       "  0.7699999809265137,\n",
       "  0.7766666412353516,\n",
       "  0.7666666507720947,\n",
       "  0.7766666412353516,\n",
       "  0.7633333206176758,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.7699999809265137,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.7699999809265137,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.7666666507720947,\n",
       "  0.7699999809265137,\n",
       "  0.7599999904632568,\n",
       "  0.7766666412353516,\n",
       "  0.7733333110809326,\n",
       "  0.7566666603088379,\n",
       "  0.7699999809265137,\n",
       "  0.7633333206176758,\n",
       "  0.7599999904632568,\n",
       "  0.7733333110809326,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7766666412353516,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7766666412353516,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7699999809265137,\n",
       "  0.7566666603088379,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7633333206176758,\n",
       "  0.7566666603088379,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7633333206176758,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.75,\n",
       "  0.7566666603088379,\n",
       "  0.75,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.75,\n",
       "  0.7566666603088379,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.753333330154419,\n",
       "  0.7633333206176758,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.7666666507720947,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.7633333206176758,\n",
       "  0.7666666507720947,\n",
       "  0.753333330154419,\n",
       "  0.7633333206176758,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.753333330154419,\n",
       "  0.753333330154419,\n",
       "  0.7633333206176758,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.75,\n",
       "  0.7633333206176758,\n",
       "  0.75,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.75,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.7566666603088379,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.753333330154419,\n",
       "  0.7566666603088379,\n",
       "  0.7566666603088379,\n",
       "  0.7666666507720947,\n",
       "  0.7766666412353516,\n",
       "  0.7699999809265137,\n",
       "  0.7733333110809326,\n",
       "  0.7666666507720947,\n",
       "  0.7633333206176758,\n",
       "  0.7699999809265137,\n",
       "  0.7566666603088379,\n",
       "  0.7566666603088379,\n",
       "  0.7666666507720947,\n",
       "  0.7566666603088379,\n",
       "  0.7666666507720947,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7566666603088379,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419,\n",
       "  0.7633333206176758,\n",
       "  0.7633333206176758,\n",
       "  0.753333330154419,\n",
       "  0.7599999904632568,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7633333206176758,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.75,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7566666603088379,\n",
       "  0.7599999904632568,\n",
       "  0.7599999904632568,\n",
       "  0.753333330154419])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "test_accuracy, test_data, test_labels = test(tensor_test_mask)\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "low_dim_embs = tsne.fit_transform(test_data)\n",
    "plt.title('tsne result')\n",
    "plt.scatter(low_dim_embs[:,0], low_dim_embs[:,1], marker='o', c=test_labels)\n",
    "plt.savefig(\"tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
